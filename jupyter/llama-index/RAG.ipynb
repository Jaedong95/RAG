{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffded25f-9434-4e07-84f3-389c6f564946",
   "metadata": {},
   "source": [
    "### Default Setting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "262bafbd-633e-4c2f-941a-c1d85ab53f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "from llama_index.readers.chroma import ChromaReader\n",
    "from llama_index import StorageContext, load_index_from_storage, load_indices_from_storage\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "from llama_index.llms import HuggingFaceLLM\n",
    "from llama_index.node_parser import SentenceSplitter \n",
    "from llama_index.schema import MetadataMode\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index import QueryBundle \n",
    "import chromadb\n",
    "import pandas as pd \n",
    "import openai\n",
    "import os\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ee6d336-2a4d-48b4-a386-a3119de2893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join('/workspace/data/')\n",
    "index_path = os.path.join('/workspace/db/local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9b1655d-60d6-45b5-ad8c-4c1028e4bc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OpenAI API Key: ········\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfb82ce-72b4-4bcd-b6ba-c504170999e8",
   "metadata": {},
   "source": [
    "### Retrieve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6fadcd48-c8cd-4222-9fa4-5cba58dc2d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers import VectorIndexRetriever \n",
    "from llama_index.vector_stores import SimpleVectorStore\n",
    "from llama_index.query_engine import RetrieverQueryEngine \n",
    "from llama_index.postprocessor import SimilarityPostprocessor \n",
    "from llama_index.postprocessor import SentenceTransformerRerank \n",
    "from llama_index.postprocessor import KeywordNodePostprocessor \n",
    "from llama_index.postprocessor import SimilarityPostprocessor, CohereRerank\n",
    "from llama_index.schema import Node, NodeWithScore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "beffcdb9-7b21-434e-a300-a8872bab7aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_vectorstore = SimpleVectorStore.from_persist_path(os.path.join(index_path, 'desc', 'default__vector_store.json'))\n",
    "features_vectorstore = SimpleVectorStore.from_persist_path(os.path.join(index_path, 'features', 'default__vector_store.json'))\n",
    "qualification_vectorstore = SimpleVectorStore.from_persist_path(os.path.join(index_path, 'qualification', 'default__vector_store.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a9073dab-de5a-4563-8448-1a071d83452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_dict = desc_vectorstore.to_dict()\n",
    "features_dict = features_vectorstore.to_dict()\n",
    "qualification_dict = qualification_vectorstore.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e7749e-e6db-4431-a3ae-0af2c6ab04b3",
   "metadata": {},
   "source": [
    "#### - Embedding Model  - kakaobank "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ad3cd99-127b-44a5-b1c5-e9464acfe19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'kakaobank/kf-deberta-base'\n",
    "embed_model = HuggingFaceEmbedding(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e28d5df-9e3f-4b17-804a-c562e426ba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_storage_context = StorageContext.from_defaults(persist_dir=os.path.join(index_path, 'desc'))\n",
    "features_storage_context = StorageContext.from_defaults(persist_dir=os.path.join(index_path, 'features'))\n",
    "qualification_storage_context = StorageContext.from_defaults(persist_dir=os.path.join(index_path, 'qualification'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e30eb0d-5dc0-48f8-a679-6d6a162653c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = SentenceSplitter(chunk_size=512, chunk_overlap=30)   # SentenceSplitter(chunk_size=1024, chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e71c2a1-755f-4f54-a7de-87b9c6fdae88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "embedding_service = ServiceContext.from_defaults(node_parser=parser, embed_model=embed_model, llm=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e900954-4aae-41c2-81a8-4df0307de301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# service_context 전달 안해주면 query 시 dimension 오류 발생 \n",
    "features_idx = load_index_from_storage(features_storage_context, index_id='loan_tmp', service_context=embedding_service)\n",
    "desc_idx = load_indices_from_storage(desc_storage_context, index_ids=['card_tmp', 'loan_tmp', 'deposit_tmp'], service_context=embedding_service)\n",
    "qualification_idx = load_index_from_storage(qualification_storage_context, index_id='loan_tmp', service_context=embedding_service)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdd2814-0bc2-4fb1-9abf-aa75c7b99a8f",
   "metadata": {},
   "source": [
    "#### - Retriever "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "def9d677-fa89-4431-bd23-927785be9cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = VectorIndexRetriever(\n",
    "    index = features_idx,\n",
    "    service_context=embedding_service,\n",
    "    similarity_top_k = 10, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34e2ffe9-f62f-41a1-b125-c4dd5ba8761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retrieved_nodes(\n",
    "    retriever, query_str, vector_top_k=10, similarity_cutoff=0.6, reranker_top_n=3, service_context=None, with_reranker=False\n",
    "):\n",
    "    # query bundle 생성 \n",
    "    query_bundle = QueryBundle(query_str)\n",
    "\n",
    "    # 유사도가 제일 높은 node 추출 \n",
    "    retrieved_nodes = retriever.retrieve(query_bundle)\n",
    "\n",
    "    # 전처리  - 유사 점수 기준 Cutoff \n",
    "    node_postprocessors = SimilarityPostprocessor(similarity_cutoff=similarity_cutoff)\n",
    "    processed_nodes = node_postprocessors.postprocess_nodes(retrieved_nodes)\n",
    "    \n",
    "    if with_reranker:   # 재순위화 \n",
    "        reranker = SentenceTransformerRerank(\n",
    "            model='bongsoo/albert-small-kor-cross-encoder-v1',\n",
    "            top_n=reranker_top_n,\n",
    "        )\n",
    "        reranked_nodes = reranker.postprocess_nodes(\n",
    "            processed_nodes, query_bundle\n",
    "        )\n",
    "    return reranked_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "80cd1d70-e5da-44bb-87d4-11ba9db5a14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_reranker = True\n",
    "cutoff = 0.4\n",
    "query = '대출 금리가 가장 낮은 상품 정보 알려줘'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7b11ab82-8b4f-4f23-a254-6ab257e06719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8045570850372314\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "retrieved_nodes = retriever.retrieve(query)\n",
    "nodes = get_retrieved_nodes(retriever, query, similarity_cutoff=cutoff, service_context=embedding_service, with_reranker=with_reranker)\n",
    "end = time.time() - start \n",
    "print(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fffaf902-2be6-47f0-81b3-a722ad4f31f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(node):\n",
    "    '''\n",
    "    id, text, score 반환 \n",
    "    '''\n",
    "    id = features_dict['metadata_dict'][node.node_id]['document_id']\n",
    "    txt = node.text \n",
    "    score = node.score \n",
    "    return [id, txt, score] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "55be9531-185d-4d01-a68f-3771b1502977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['주택담보대출 이율변동이 걱정되시나요? 처음 금리가 대출만기까지 고정되는 순수장기고정금리 상품을 만나보세요!!', '주택담보대출 이율변동이 걱정되시나요? 처음 금리가 대출만기까지 고정되는 순수장기고정금리 상품을 만나보세요!!', '최장 50년간 고정금리 상품으로 금리변동에 따른 위험부담이 없으며, 아파트의 경우 소액임차보증금 공제없이 집값의 최대 70%까지 대출이 가능한 상품입니다.-기본형 : 대출실행일부터 만기까지 고정금리가 적용되는 상품-주택연금사전예약형 : 만40세 이상(본인 또는 배우자)고객이 보금자리론을 이용하면서 주택연금 가입을 사전예약하고 주택연금으로 전환 시 우대금리(연0.20%) 누적액을 장려금으로 지급하는 상품-입주자전용 : 신규 분양아파트 잔금대출 용도로 다음 요건을 충족하는 고객이 신청 가능*요건:1.(임시)사용승인 완료2.신청인이 최초 입주예정자(등기사항전부증명서상 최초 소유권자)3.당행 입주잔금대출 승인을 받은 사업장(신청 영업점에 문의)', '대출실행일부터 만기까지 안정적인 고정금리가 적용되는 상품으로 향후 금리 변동의 위험을 피하고자 하는 고객에게 적합한 상품개인', '변동금리 수준으로 금리우대 및 기준금리 변경 옵션을 부여한 대출상품', '당행 주택담보대출의 잔액만큼 대환가능한 상품으로서 고객의 대출금리를 낮춘 장기 고정금리 상품으로 향후 금리 변동의 위험을 피하고자 하는 고객에게 적합한 상품(인터넷 비대면 약정)', '담보대출을 사용하면 입출금통장에 대출이율과 같은 이율을 적용하여 최대 50%의 대출이자 절감효과(세전)를 누릴 수 있는 신개념 담보대출상품', '고정금리의 혜택과 변동금리의 혜택을 동시에 누릴수 있는 주택담보대출', '대출실행일부터 만기까지 안정적인 고정금리 주택담보대출.향후 금리 변동의 위험을 피하고자 하는 고객에게 적합한 상품.개인', '처음금리가 대출만기까지 고정되는 고정금리 주택자금대출이 부담되시나요? 5년마다 금리가 조정되는 New 순수장기고정금리 상품을 만나보세요!!']\n"
     ]
    }
   ],
   "source": [
    "print([node.text for node in retrieved_nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "875b4b94-2965-4267-b978-65d2c4522a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['변동금리 수준으로 금리우대 및 기준금리 변경 옵션을 부여한 대출상품',\n",
       "  '주택담보대출 이율변동이 걱정되시나요? 처음 금리가 대출만기까지 고정되는 순수장기고정금리 상품을 만나보세요!!',\n",
       "  '주택담보대출 이율변동이 걱정되시나요? 처음 금리가 대출만기까지 고정되는 순수장기고정금리 상품을 만나보세요!!'],\n",
       " [0.23542526, 0.21586773, 0.21586773])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts = [node.text for node in nodes]\n",
    "scores = [node.score for node in nodes]\n",
    "contexts, scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7d091bf1-a89c-468c-a83f-a9a4dd1db691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['대출_510', '변동금리 수준으로 금리우대 및 기준금리 변경 옵션을 부여한 대출상품', 0.23542526]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_info(nodes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4984bc-3673-43ce-b38a-27cd2c9f43ca",
   "metadata": {},
   "source": [
    "### Augment "
   ]
  },
  {
   "cell_type": "raw",
   "id": "913561f2-6d41-4c25-a19a-373033cabb6f",
   "metadata": {},
   "source": [
    "from llama_index.llms import ChatMessage, MessageRole\n",
    "from llama_index.prompts import ChatPromptTemplate\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, AutoPromptTemplate\n",
    "\n",
    "# Text QA Prompt\n",
    "chat_text_qa_msgs = [\n",
    "    ChatMessage(\n",
    "        role=MessageRole.USER,\n",
    "        content=(\n",
    "            \"사용자 쿼리와 유사한 문맥 정보는 다음과 같아. \\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context_str}\"\n",
    "            \"\\n---------------------\\n\"\n",
    "            \"주어진 문맥 정보를 바탕으로, 사용자 질문에 대답해줘: {query_str}\\n\"\n",
    "            \"대답은 한국어로 하고, 주어진 문맥이 별로 도움되지 않는다면, 너가 대답을 만들어줘\"\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "text_qa_template = ChatPromptTemplate(chat_text_qa_msgs)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bd56aa65-40a0-46a8-afa2-61158f068169",
   "metadata": {},
   "source": [
    "chat_refine_msgs = [\n",
    "    ChatMessage(\n",
    "        role=MessageRole.SYSTEM,\n",
    "        content=(\n",
    "            \"주어진 문맥 정보가 유용하지 않더라도 항상 사용자 발화에 대답해줘\"\n",
    "        ),\n",
    "    ),\n",
    "    ChatMessage(\n",
    "        role=MessageRole.USER,\n",
    "        content=(\n",
    "            \"우리는 기존 대답을 정제할 기회가 있어\"\n",
    "            \"(꼭 필요한 경우에만) 다음 추가 문맥 정보를 사용해서.\\n\"\n",
    "            \"------------\\n\"\n",
    "            \"{context_msg}\\n\"\n",
    "            \"------------\\n\"\n",
    "            \"새로운 문맥 정보가 주어졌을 때, 기존 대답을 보다 낫게 정제해봐\"\n",
    "            \"대답해야 할 사용자 발화는 이거야: {query_str}. \"\n",
    "            \"추가로 주어진 문맥 정보가 유용하지 않다면, 기존 대답을 출력해줘.\\n\"\n",
    "            \"기존 대답: {existing_answer}\"\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "refine_template = ChatPromptTemplate(chat_refine_msgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9cb4d4fd-fb8e-4cde-922a-6e48e3533626",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = f\"\"\"\n",
    "    사용자 쿼리와 유사한 문맥 정보는 다음과 같아. \n",
    "    ---------------------\\n\n",
    "    {contexts[0]}\n",
    "    \\n---------------------\\n\n",
    "    주어진 문맥 정보를 바탕으로, 사용자 질문에 대답해줘: {query}\\n\n",
    "    대답은 한국어로 하고, 주어진 문맥이 별로 도움되지 않는다면, 너가 대답을 만들어줘\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa30302-57f0-4daa-bb0b-6b252a0ce1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "messages = [\n",
    "{\"role\": \"user\", \"content\": \"bbbbbb\"},\n",
    "{\"role\": \"assistant\", \"content\" : \"aaaaa\"},\n",
    "{\"role\": \"user\", \"content\" : \"bbbb\"},\n",
    "{\"role\": \"assistant\", \"content\" : \"aaaaa\"},\n",
    "{\"role\": \"user\", \"content\" : \"bbbbb\"},\n",
    "{\"role\": \"assistant\", \"content\" : \"aaaaa\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141ff1cc-ec6a-41d1-b65d-b7f03b20b497",
   "metadata": {},
   "source": [
    "### Generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "14dcd096-cbeb-4231-93bc-12c11d657140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import TextStreamer, GenerationConfig\n",
    "from llama_index.response_synthesizers import (\n",
    "    ResponseMode,\n",
    "    get_response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "66486101-39ef-4745-bdf6-a3b580a2494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8642bd11-0837-472f-a54d-409084248719",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35fe3a8715d94f96903b7cbe35628485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = HuggingFaceLLM(\n",
    "    context_window=512,\n",
    "    max_new_tokens=128,\n",
    "    generate_kwargs={\"temperature\": 1.0, \"do_sample\": True},\n",
    "     tokenizer_name=\"davidkim205/komt-mistral-7b-v1\",\n",
    "    model_name=\"davidkim205/komt-mistral-7b-v1\",\n",
    "    device_map=\"cuda\",\n",
    "    # offload_folder='offload',\n",
    "    stopping_ids=[50278, 50279, 50277, 1, 0],\n",
    "    tokenizer_kwargs={\"max_length\": 1024},\n",
    "    # uncomment this if using CUDA to reduce memory usage\n",
    "    # model_kwargs={\"torch_dtype\": torch.float16}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "07df4691-4266-417c-940e-8aacf56989de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mistral_service_context = ServiceContext.from_defaults(node_parser=parser, llm=llm)\n",
    "\n",
    "responser = get_response_synthesizer(\n",
    "    response_mode=\"compact\",\n",
    "    service_context=mistral_service_context,\n",
    "    verbose=True,\n",
    "    structured_answer_filtering=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b056563b-0d60-46dc-b480-e64f83729d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['변동금리 수준으로 금리우대 및 기준금리 변경 옵션을 부여한 대출상품',\n",
       " '주택담보대출 이율변동이 걱정되시나요? 처음 금리가 대출만기까지 고정되는 순수장기고정금리 상품을 만나보세요!!',\n",
       " '주택담보대출 이율변동이 걱정되시나요? 처음 금리가 대출만기까지 고정되는 순수장기고정금리 상품을 만나보세요!!']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts"
   ]
  },
  {
   "cell_type": "raw",
   "id": "17e2969a-1fe4-4ac5-925b-de272913c6af",
   "metadata": {},
   "source": [
    "response = responser.get_response(query, contexts)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "935f277e-9480-4cf2-a7df-3a45eb49f978",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=responser,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.3)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b3c3722c-3664-4c99-aad4-e124eed9be39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'대출 금리가 가장 낮은 상품 정보 알려줘'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "839a3183-7e33-45ed-b9bd-f0f0584a09a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = f\"[INST] + {query} + [/INST]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "77480b45-dc13-4832-8544-fc12297f50cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Refine context: 입출금통장에 대출이율과 같은 이율을 적용하여 최대 50%의 대출이자 절감효과(세전)를...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not extract json string from output: \nExamples:\nGiven:\n\"You have found a list of loan companies online. Compare their loan products to see which ones offer the best rates. Each company has given you their respective interest rates for different loan terms. Which lender offers the lowest monthly payment for a $5000 loan term?\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mquery_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/llama_index/core/base_query_engine.py:40\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     39\u001b[0m     str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_or_query_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/llama_index/query_engine/retriever_query_engine.py:172\u001b[0m, in \u001b[0;36mRetrieverQueryEngine._query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    169\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mQUERY, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_bundle\u001b[38;5;241m.\u001b[39mquery_str}\n\u001b[1;32m    170\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m query_event:\n\u001b[1;32m    171\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve(query_bundle)\n\u001b[0;32m--> 172\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_response_synthesizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynthesize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_bundle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m     query_event\u001b[38;5;241m.\u001b[39mon_end(payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mRESPONSE: response})\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/llama_index/response_synthesizers/base.py:168\u001b[0m, in \u001b[0;36mBaseSynthesizer.synthesize\u001b[0;34m(self, query, nodes, additional_source_nodes, **response_kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m     query \u001b[38;5;241m=\u001b[39m QueryBundle(query_str\u001b[38;5;241m=\u001b[39mquery)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    166\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mSYNTHESIZE, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query\u001b[38;5;241m.\u001b[39mquery_str}\n\u001b[1;32m    167\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[0;32m--> 168\u001b[0m     response_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_chunks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMetadataMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnodes\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m     additional_source_nodes \u001b[38;5;241m=\u001b[39m additional_source_nodes \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[1;32m    177\u001b[0m     source_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(nodes) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(additional_source_nodes)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/llama_index/response_synthesizers/compact_and_refine.py:38\u001b[0m, in \u001b[0;36mCompactAndRefine.get_response\u001b[0;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# use prompt helper to fix compact text_chunks under the prompt limitation\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# TODO: This is a temporary fix - reason it's temporary is that\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# the refine template does not account for size of previous answer.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m new_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_compact_text_chunks(query_str, text_chunks)\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_chunks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_texts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprev_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprev_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/llama_index/response_synthesizers/refine.py:151\u001b[0m, in \u001b[0;36mRefine.get_response\u001b[0;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_give_response_single(\n\u001b[1;32m    147\u001b[0m             query_str, text_chunk, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs\n\u001b[1;32m    148\u001b[0m         )\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;66;03m# refine response if possible\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_refine_response_single\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprev_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kwargs\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m     prev_response \u001b[38;5;241m=\u001b[39m response\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/llama_index/response_synthesizers/refine.py:284\u001b[0m, in \u001b[0;36mRefine._refine_response_single\u001b[0;34m(self, response, query_str, text_chunk, **response_kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_streaming:\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m         structured_response \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m    283\u001b[0m             StructuredRefineResponse,\n\u001b[0;32m--> 284\u001b[0m             \u001b[43mprogram\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcontext_msg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_text_chunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    288\u001b[0m         )\n\u001b[1;32m    289\u001b[0m         query_satisfied \u001b[38;5;241m=\u001b[39m structured_response\u001b[38;5;241m.\u001b[39mquery_satisfied\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m query_satisfied:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/llama_index/program/llm_program.py:103\u001b[0m, in \u001b[0;36mLLMTextCompletionProgram.__call__\u001b[0;34m(self, llm_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm\u001b[38;5;241m.\u001b[39mcomplete(formatted_prompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mllm_kwargs)\n\u001b[1;32m    101\u001b[0m     raw_output \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m--> 103\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_output_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_cls):\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput parser returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(output)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_cls\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/llama_index/output_parsers/pydantic.py:61\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     60\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Parse, validate, and correct errors programmatically.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m     json_str \u001b[38;5;241m=\u001b[39m \u001b[43mextract_json_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_cls\u001b[38;5;241m.\u001b[39mparse_raw(json_str)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/llama_index/output_parsers/utils.py:112\u001b[0m, in \u001b[0;36mextract_json_str\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    110\u001b[0m match \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m.*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m, text\u001b[38;5;241m.\u001b[39mstrip(), re\u001b[38;5;241m.\u001b[39mMULTILINE \u001b[38;5;241m|\u001b[39m re\u001b[38;5;241m.\u001b[39mIGNORECASE \u001b[38;5;241m|\u001b[39m re\u001b[38;5;241m.\u001b[39mDOTALL)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m match:\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not extract json string from output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m match\u001b[38;5;241m.\u001b[39mgroup()\n",
      "\u001b[0;31mValueError\u001b[0m: Could not extract json string from output: \nExamples:\nGiven:\n\"You have found a list of loan companies online. Compare their loan products to see which ones offer the best rates. Each company has given you their respective interest rates for different loan terms. Which lender offers the lowest monthly payment for a $5000 loan term?\""
     ]
    }
   ],
   "source": [
    "response = query_engine.query(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d586d9c-4fa3-421c-8807-b72077d1c504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30ba4b5-caaa-4b90-be96-495879426602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a0deb7-b6a4-45aa-b88d-a55fb373eb89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf273a49-723d-4331-a824-74a232315ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "873885cc-3d06-4904-8963-5ca901c60bd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdavidkim205/komt-mistral-7b-v1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mconfig\u001b[49m\u001b[38;5;241m.\u001b[39mbase_model_name_or_path \u001b[38;5;241m=\u001b[39m model_name\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(config\u001b[38;5;241m.\u001b[39mbase_model_name_or_path, quantization_config\u001b[38;5;241m=\u001b[39mbnb_config, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "model_name='davidkim205/komt-mistral-7b-v1'\n",
    "config.base_model_name_or_path = model_name\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, quantization_config=bnb_config, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "32efa8a5-a131-4874-8bae-311f6cc128ea",
   "metadata": {},
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d711ac36-e685-461f-8547-cb448ef0efa3",
   "metadata": {},
   "source": [
    "model='davidkim205/komt-mistral-7b-v1'\n",
    "peft_model_name = 'davidkim205/komt-mistral-7b-v1-lora'\n",
    "config = PeftConfig.from_pretrained(peft_model_name)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "config.base_model_name_or_path = model\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, quantization_config=bnb_config, device_map=\"auto\")\n",
    "model = PeftModel.from_pretrained(model, peft_model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "streamer = TextStreamer(tokenizer)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9782cf98-02f5-4e91-8028-1d2a716c9e34",
   "metadata": {},
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afde3928-023e-42ab-a079-9cb32b5b5d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    temperature=1,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    max_new_tokens=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757bd42c-666f-4198-9423-5274eccd3269",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gened = model.generate(\n",
    "    **tokenizer(\n",
    "        chat_template,\n",
    "        return_tensors='pt',\n",
    "        return_token_type_ids=False\n",
    "    ).to('cuda'),\n",
    "    generation_config=generation_config,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    streamer=streamer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40a64e8-e313-49ad-a1eb-a837601e5410",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_str = tokenizer.decode(gened[0])\n",
    "\n",
    "start_tag = f\"[/INST]\"\n",
    "start_index = result_str.find(start_tag)\n",
    "\n",
    "if start_index != -1:\n",
    "    result_str = result_str[start_index + len(start_tag):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4377e4a7-b56b-455c-96e4-5c0fb013f0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336c17f1-0d39-45f1-a16d-5e8e4d527f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = '블랙-숄즈 모델(Black-Scholes Model)이 무엇인지 설명해주세요.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc40078-060a-4b3a-b034-a1b1c019ff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_str = contexts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18009d3c-6f0b-425e-81e2-2f5339d7ea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = gen(contexts[0])\n",
    "\n",
    "print('##########')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba539783-0507-44a0-8977-79ca8cda50bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
