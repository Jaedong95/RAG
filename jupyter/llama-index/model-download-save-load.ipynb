{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c91c9085-bb3e-4191-be43-88dd7b2e6ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import getpass\n",
    "import torch\n",
    "import os \n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, BitsAndBytesConfig, AutoTokenizer, AutoConfig\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0930c33-1033-47d6-913a-5c60f4f25927",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_path = os.getcwd()\n",
    "data_path = os.path.join(default_path, '../../data')\n",
    "model_path = os.path.join(default_path, '../../model')\n",
    "config_path = os.path.join(default_path, '../../config')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c47534ff-63d5-4c69-b837-ee4c3f634a75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "928c3466-ef97-46d4-9f8a-cdc6aa20131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = dict(\n",
    "    temperature=0.8,\n",
    "    # top_k=40,\n",
    "    top_p=0.95,\n",
    "    # do_sample=True,\n",
    "    max_new_tokens=512,\n",
    "    # early_stopping=True,\n",
    "    # no_repeat_ngram_size=3,\n",
    "    # eos_token_id=2, \n",
    "    # num_beams=1,\n",
    "    # repetition_penalty=1.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b36105e9-0888-4ae3-92bb-b801566200ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(x, config, model, tokenizer, device):\n",
    "    prompt = (\n",
    "        f\"아래는 작업을 설명하는 명령어입니다. 요청을 적절히 완료하는 응답을 작성하세요.\\n\\n### 명령어:\\n{x}\\n\\n### 응답:\"\n",
    "    )\n",
    "    len_prompt = len(prompt)\n",
    "    gened = model.generate(\n",
    "        **tokenizer(prompt, return_tensors=\"pt\", return_token_type_ids=False).to(\n",
    "            device\n",
    "        ),\n",
    "        **config\n",
    "    )\n",
    "    return tokenizer.decode(gened[0]) # [len_prompt:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1e191b0-944e-4a62-b830-c3841e6d6aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"kfkas/Llama-2-ko-7b-Chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47c52cfb-d8c0-49f5-acc3-13e9d3a82ca8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1d5a498c2ec47bdbe67fdf30467f709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_id, device_map={\"\": 0},torch_dtype=torch.float16, low_cpu_mem_usage=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877a9d75-eb4f-4a8c-a58e-069009f6e5bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
