{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c91c9085-bb3e-4191-be43-88dd7b2e6ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import getpass\n",
    "import torch\n",
    "import os \n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, LlamaTokenizerFast, BitsAndBytesConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0930c33-1033-47d6-913a-5c60f4f25927",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_path = os.getcwd()\n",
    "data_path = os.path.join(default_path, '../../data')\n",
    "model_path = os.path.join(default_path, '../../models')\n",
    "config_path = os.path.join(default_path, '../../config')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c47534ff-63d5-4c69-b837-ee4c3f634a75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2866d9cc-fc56-4395-aa00-e3275b3775da",
   "metadata": {},
   "source": [
    "### Download Llama2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e191b0-944e-4a62-b830-c3841e6d6aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"kfkas/Llama-2-ko-7b-Chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e5b59f-cff7-4895-8e9f-cb7ec4ed9ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizerFast.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c52cfb-d8c0-49f5-acc3-13e9d3a82ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizerFast.from_pretrained(model_id)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_id, device_map={\"\": 0},torch_dtype=torch.float16, low_cpu_mem_usage=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415cc4e6-0298-4766-a8c6-17f7e8e7e6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(os.path.join(model_path, 'llama2_origin_tok'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef9ca36-6dd6-428e-b574-18612185f804",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer), tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd24288-ca08-4daa-8fe6-8afaa888d751",
   "metadata": {},
   "source": [
    "### Save Llama2 Model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d5227de9-33c9-492d-bf7e-aefa7417e77f",
   "metadata": {},
   "source": [
    "torch.save(model, os.path.join(model_path, 'llama2.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d7204c-749c-4ac8-a3b9-524c655c92ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(save_directory=os.path.join(model_path, \"llama2_origin\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa00743-f933-442c-b460-97d1b2e72fa3",
   "metadata": {},
   "source": [
    "### Load Llama2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0472edfb-e948-4ad6-9b69-c2e5452c9fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = os.path.join(model_path, 'llama2_origin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acd8798-72b0-4ace-bee1-3311f1a930d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de569322-39f2-46b8-abc8-f787ee1de31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoTokenizer = AutoTokenizer.from_pretrained(os.path.join(model_dir, 'tokenizer'))\n",
    "LlamaTokenizer = LlamaTokenizerFast.from_pretrained(os.path.join(model_dir, 'tokenizer'))   # LlamaTokenizer (x)  -> LlamaTokenizerFast (o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17511d59-a2fe-46c3-af26-2c50fc659be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(AutoTokenizer.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6224a0-71ad-4ddc-8007-f926c38cd8de",
   "metadata": {},
   "source": [
    "### Download Mistral Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14630136-7606-4eab-bf07-44d81fae1ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='davidkim205/komt-mistral-7b-v1'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, low_cpu_mem_usage=True) # , device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4d2948-2e08-482a-ad25-317067cdfb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23605119-45db-4e34-8ecd-a1d048b2d8ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35aabad2-bb4b-4781-bf31-82db67e208f0",
   "metadata": {},
   "source": [
    "### Save Mistral Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3237bc-aab6-425e-9c5d-b2ffa785094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(save_directory=os.path.join(model_path, \"mistral_origin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d593688c-f27c-4d65-b7e4-33f35a2df0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(os.path.join(model_path, 'mistral_origin', 'tokenizer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6f35ff-29c5-43f1-a7bb-9ed197f7d2ff",
   "metadata": {},
   "source": [
    "### Load Mistral Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81050e8e-a4c3-445a-837f-3dbcc45a756b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = os.path.join(model_path, \"mistral_origin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ecdec28-1111-4dae-a170-979af363aa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizerFast.from_pretrained(os.path.join(model_dir, 'tokenizer'))   # LlamaTokenizer (x)  -> LlamaTokenizerFast (o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d711d630-4eff-4577-ba74-2d6eb3efe2b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9af323013c546799e6535fab8b7028d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_dir, torch_dtype=torch.float16, low_cpu_mem_usage=True) # , device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a8e2efa-38e6-4ca9-a8d6-ca3954f80f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be39134-c230-48f2-9c6c-0b18ce4373fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
