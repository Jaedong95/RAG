{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c2bc361-b1c6-4a58-ac7c-84343845bd78",
   "metadata": {},
   "source": [
    "### Langchain + RAG Demo on LlaMa-2–7b + Embeddings Model using Chainlit \n",
    "##### https://medium.com/@madhur.prashant7/demo-langchain-rag-demo-on-llama-2-7b-embeddings-model-using-chainlit-559c10ce3fbf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23059298-2604-4e42-9d96-1e8ae5ce2c71",
   "metadata": {},
   "source": [
    "### 0. Default Setting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "245c71b6-fe8e-490e-b82d-c4eea30fa20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainlit as cl\n",
    "import transformers\n",
    "import torch\n",
    "import os \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain import PromptTemplate\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import CTransformers\n",
    "from langchain.chains import RetrievalQA\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab611e55-4e47-4515-ab87-e33308577940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.2'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1ce6fc2-a152-4545-8d0f-2bc6c36ba995",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_path = os.getcwd()\n",
    "data_path = os.path.join(default_path, '../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e836756-04be-498f-a4dd-828376ecd7f2",
   "metadata": {},
   "source": [
    "### 1. Env Setting"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a0b1f648-a63b-4425-822a-dc88bcda10b4",
   "metadata": {},
   "source": [
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1a2c1dcd-bb06-4314-97e3-6d39ae968a57",
   "metadata": {},
   "source": [
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b7c62556-19fe-4d9f-896f-9a88b0eeb588",
   "metadata": {},
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90f8b94-e49b-4a2f-8d8c-d56bb31ad2f8",
   "metadata": {},
   "source": [
    "### 2. Chroma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cacd071-fe65-430b-9f91-70ada8ff2492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4cfebb7-1172-4e06-890d-a7c50a51d61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-26 04:56:21 - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2023-12-26 04:56:21 - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    }
   ],
   "source": [
    "# 연결 테스트\n",
    "chroma_client = chromadb.HttpClient(host='172.19.240.1', port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2386c279-7d81-4fcb-9291-f0fbd6692bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_client.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4cab944-f79d-4565-ad0c-2d1321fe0813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_prompt():\n",
    "    \"\"\"\n",
    "    Prompt template for QA retrieval for each vectorstore\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(template=custom_prompt_template,\n",
    "                            input_variables=['context', 'question'])\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97a94d16-8fde-436b-a9d5-e2172849edd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_qa_chain(llm, prompt, db):\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                       chain_type='stuff',\n",
    "                                       retriever=db.as_retriever(search_kwargs={'k': 2}),\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type_kwargs={'prompt': prompt}\n",
    "                                       )\n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "289785ec-388a-45ed-81aa-5f3703edeaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the model\n",
    "def load_llm():\n",
    "    # Load the locally downloaded model here\n",
    "    llm = CTransformers(\n",
    "        model = \"meta/Llama-2-7B-Chat-GGML\",\n",
    "        model_type=\"llama\",\n",
    "        max_new_tokens = 512,\n",
    "        temperature = 0.5\n",
    "    )\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9ce28ad-9bc6-4ac0-b93f-298f42fc1e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#QA Model Function\n",
    "def qa_bot():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                                       model_kwargs={'device': 'cpu'})\n",
    "    db = FAISS.load_local(DB_FAISS_PATH, embeddings)\n",
    "    llm = load_llm()\n",
    "    qa_prompt = set_custom_prompt()\n",
    "    qa = retrieval_qa_chain(llm, qa_prompt, db)\n",
    "    return qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f201c21d-104d-4791-89a0-8c531589f7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_result(query):\n",
    "    qa_result = qa_bot()\n",
    "    response = qa_result({'query': query})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27732f1d-40e1-4352-88a3-1adfbaea7a55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
