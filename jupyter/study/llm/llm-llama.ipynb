{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22699782-8ba2-4d60-b5c4-68453b81b291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import os \n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, BitsAndBytesConfig, AutoTokenizer, AutoConfig\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89d55faa-f129-4706-b6ac-a5d68b1ead03",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_tok = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81eee46b-5b0e-4278-994f-b35c32a8462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_path = os.getcwd()\n",
    "data_path = os.path.join(default_path, '../../data')\n",
    "model_path = os.path.join(default_path, '../../model')\n",
    "config_path = os.path.join(default_path, '../../config')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "796e76cb-5dad-4808-82d9-b39c3caf1eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4225cae7-d5ba-47f8-b852-410866e74c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = dict(\n",
    "    temperature=0.8,\n",
    "    # top_k=40,\n",
    "    top_p=0.95,\n",
    "    # do_sample=True,\n",
    "    max_new_tokens=512,\n",
    "    # early_stopping=True,\n",
    "    # no_repeat_ngram_size=3,\n",
    "    # eos_token_id=2, \n",
    "    # num_beams=1,\n",
    "    # repetition_penalty=1.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "023e1f7b-74b9-467f-9ca1-a12bdd9b8a85",
   "metadata": {},
   "source": [
    "generation_config['num_beams']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c383a981-f1ea-4574-a786-637fe57f3a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(x, config, model, tokenizer, device):\n",
    "    prompt = (\n",
    "        f\"아래는 작업을 설명하는 명령어입니다. 요청을 적절히 완료하는 응답을 작성하세요.\\n\\n### 명령어:\\n{x}\\n\\n### 응답:\"\n",
    "    )\n",
    "    len_prompt = len(prompt)\n",
    "    gened = model.generate(\n",
    "        **tokenizer(prompt, return_tensors=\"pt\", return_token_type_ids=False).to(\n",
    "            device\n",
    "        ),\n",
    "        **config\n",
    "    )\n",
    "    return tokenizer.decode(gened[0]) # [len_prompt:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8634f80a-0055-496b-b01d-ef264bf1d9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"kfkas/Llama-2-ko-7b-Chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06317844-fe7a-484a-81e2-2f914efb6874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa6b5d69016640c78600aed7360a6909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/698 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "059cf78116df4649a589bdc780128616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/5.21M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/kfkas/Llama-2-ko-7b-Chat/resolve/main/tokenizer.json: HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b847dfdde245f69af96e9078580321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/5.21M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "695f5fb73ab04af99c508bac3683c206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e28782e377b4787974917a78ce9864a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/647 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9152953d1aac446181a159fe75749580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f46233446bd43838391c04182254061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7073e2e7b176453e9c68853ca431224f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ab1e3fe017f4212bc0aa41d29378169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.80G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2eae03956cf46b5944360802113b9d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61ddad6892b34e2ba7dcfbbd3ebba41d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_id, device_map={\"\": 0},torch_dtype=torch.float16, low_cpu_mem_usage=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37f36f0c-04f5-4b09-a3f5-9a361cf22135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(46336, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=46336, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.use_cache = (True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2951f4d-6b23-45ae-90d5-ed0ccba575da",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = '한국은행의 역할과 기능에 대해서 설명해보시요'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f7e2ac0-d9fe-47cb-8234-d0d6e36b5c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s>아래는작업을설명하는명령어입니다.요청을적절히완료하는응답을작성하세요.\\n\\n###명령어:\\n한국은행의역할과기능에대해서설명해보시요\\n\\n###응답:\\n한국은행의 역할과 기능은 다음과 같습니다:\\n\\n1. 통화 정책: 한국은행은 통화 정책을 통해 경제 성장과 물가 안정을 달성하기 위해 노력합니다. 통화 정책은 통화 공급을 조절하여 경제 활동을 촉진하거나 억제하는 데 사용됩니다.\\n\\n2. 금융 안정: 한국은행은 금융 안정을 유지하기 위해 노력합니다. 이는 금융 시스템이 안정적으로 작동하도록 보장하는 것을 의미합니다. 이를 위해 한국은행은 은행의 건전성을 감독하고, 금융 기관의 대출을 관리하며, 금융 시스템을 안정화하기 위한 정책을 시행합니다.\\n\\n3. 경제 성장: 한국은행은 경제 성장을 촉진하기 위해 노력합니다. 이는 경제 성장을 촉진하고 고용을 창출하며 소득을 증가시키는 것을 의미합니다. 이를 위해 한국은행은 경제 성장과 관련된 정책을 시행하고, 기업 활동을 촉진하며, 경제 성장을 위한 조치를 취합니다.\\n\\n4. 물가 안정: 한국은행은 물가 안정을 유지하기 위해 노력합니다. 이는 물가 상승률이 일정한 수준을 유지하도록 보장하는 것을 의미합니다. 이를 위해 한국은행은 물가 상승률을 모니터링하고, 인플레이션을 억제하기 위한 조치를 취합니다.\\n\\n5. 금융 서비스: 한국은행은 금융 서비스를 제공합니다. 이는 금융 기관이 원활하게 운영될 수 있도록 지원하는 것을 의미합니다. 이를 위해 한국은행은 금융 기관의 대출을 관리하고, 금융 서비스를 개선하며, 금융 기관이 원활하게 운영될 수 있도록 지원합니다.\\n\\n6. 금융 감독: 한국은행은 금융 감독을 수행합니다. 이는 금융 기관이 규정을 준수하고, 위험을 관리하며, 고객의 이익을 우선시하도록 보장하는 것을 의미합니다. 이를 위해 한국은행은 금융 기관의 대출을 관리하고, 금융 기관의 위험을 평가하며, 금융 기관이 규정을 준수하도록 감독합니다.\\n\\n7. 국제 협력: 한국은행은 국제 협력을 통해 경제 성장을 촉진하고, 금융 안정을 유지하며, 금융 서비스를 개선합니다. 이를 위해 한국은행은 국제 기구와 협력하여 경제 성장을 촉진하고, 금융 안정을 유지하며, 금융 서비스를 개선합니다.</s>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = gen(user_input, generation_config, model=model, tokenizer=tokenizer, device=device)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd09c175-5766-44f6-a96a-e750932740c1",
   "metadata": {},
   "source": [
    "#### PEFT Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a96a24e5-d088-4249-8e4b-a756b6eba6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "559eed0d-e955-4d5f-b8ab-8c8ab09204e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3c36795576748beb163f75f91815ab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): ModulesToSaveWrapper(\n",
       "          (original_module): Embedding(39478, 4096)\n",
       "          (modules_to_save): ModuleDict(\n",
       "            (default): Embedding(39478, 4096)\n",
       "          )\n",
       "        )\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=11008, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=11008, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=11008, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=4096, out_features=39478, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=4096, out_features=39478, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained('meta-llama/Llama-2-7b-chat-hf', token=access_tok)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    'meta-llama/Llama-2-7b-chat-hf',\n",
    "    # low_cpu_mem_usage=True,\n",
    "    quantization_config=bnb_config,\n",
    "    token=access_tok\n",
    ")\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained('Chang-Su/llama-2-7b-chat-ko')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model = PeftModel.from_pretrained(model, 'Chang-Su/llama-2-7b-chat-ko')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cfc6987-e8e3-4efa-869e-8ea07d7c942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = '삼성전자와 TSMC의 반도체 시장에서의 경쟁 상황은 어떠한가요?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dab8c16-7b8c-4657-9a3e-a05ce77e27e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start inference.\n",
      "====================\n",
      "Input: '삼성전자와 TSMC의 반도체 시장에서의 경쟁 상황은 어떠한가요?'\n",
      "\n",
      "Output: 삼성전자와 TSMC의 반도체 시장에서의 경쟁 상황은 어떠한가요?괜? 이틀? 팬들에게?�?\n",
      " 측정?\n",
      "\n",
      "You can use any of the following methods to format your citation:\n",
      "\n",
      "1. Paraphrasing: Summarize the main points of the source in your own words.\n",
      "2. Direct quotation: Use exact words from the source, enclosed in quotation marks.\n",
      "3. Parenthetical citation: Provide the author's last name and the page number(s) where the information can be found.\n",
      "4. Footnote citation: Use a superscript number in the text that links to a footnote at the bottom of the page with the full citation.\n",
      "\n",
      "For example:\n",
      "\n",
      "1. Paraphrasing:\n",
      "The newest technology from TSMC, the 3 nanometer (nm) process, is said to be \"highly competitive in terms of cost and power consumption\" (TSMC, 2022, p. 5).\n",
      "2. Direct quotation:\n",
      "According to TSMC, their new 3 nm process is \"highly competitive in terms of cost and power consumption\" (TSMC, 2022, p. 5).\n",
      "3. Parenthetical citation:\n",
      "TSMC (2022) states that their new 3 nm process is \"highly competitive in terms of cost and power consumption\" (p. 5).\n",
      "4. Footnote citation:\n",
      "TSMC (2022, p. 5) notes that their new 3 nm process is \"highly competitive in terms of cost and power consumption.\"\n",
      "\n",
      "Remember to always check the citation style guide of your institution for specific guidelines on how to format your citations.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(\"Start inference.\")\n",
    "    inputs = tokenizer(user_input,return_tensors=\"pt\")  #add_special_tokens=False ?\n",
    "    generation_output = model.generate(\n",
    "        input_ids = inputs[\"input_ids\"].to('cuda:0'),\n",
    "        attention_mask = inputs['attention_mask'].to('cuda:0'),\n",
    "        # eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        **generation_config\n",
    "    )\n",
    "    s = generation_output[0]\n",
    "    output = tokenizer.decode(s,skip_special_tokens=True)\n",
    "\n",
    "    response = output.split(\"### Response:\")[0].strip()\n",
    "    print(f\"====================\")\n",
    "    print(f\"Input: '{user_input}'\\n\")\n",
    "    print(f\"Output: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da87bb1-e8fd-4a30-96ec-3135210bc11e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
