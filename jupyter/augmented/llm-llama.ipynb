{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22699782-8ba2-4d60-b5c4-68453b81b291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import os \n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, BitsAndBytesConfig, AutoTokenizer\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89d55faa-f129-4706-b6ac-a5d68b1ead03",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_tok = '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81eee46b-5b0e-4278-994f-b35c32a8462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_path = os.getcwd()\n",
    "data_path = os.path.join(default_path, '../../data')\n",
    "model_path = os.path.join(default_path, '../../model')\n",
    "config_path = os.path.join(default_path, '../../config')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "796e76cb-5dad-4808-82d9-b39c3caf1eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4225cae7-d5ba-47f8-b852-410866e74c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = dict(\n",
    "    temperature=1,\n",
    "    top_k=40,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=200,\n",
    "    early_stopping=True,\n",
    "    no_repeat_ngram_size=3,\n",
    "    eos_token_id=2, \n",
    "    num_beams=1,\n",
    "    repetition_penalty=1.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1c469310-513c-428c-8b52-5814f38477ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_config['num_beams']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c383a981-f1ea-4574-a786-637fe57f3a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(x, config, model, tokenizer, device):\n",
    "    prompt = (\n",
    "        f\"아래는 작업을 설명하는 명령어입니다. 요청을 적절히 완료하는 응답을 작성하세요.\\n\\n### 명령어:\\n{x}\\n\\n### 응답:\"\n",
    "    )\n",
    "    len_prompt = len(prompt)\n",
    "    gened = model.generate(\n",
    "        **tokenizer(prompt, return_tensors=\"pt\", return_token_type_ids=False).to(\n",
    "            device\n",
    "        ),\n",
    "        **config\n",
    "    )\n",
    "    return tokenizer.decode(gened[0])[len_prompt:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8634f80a-0055-496b-b01d-ef264bf1d9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"kfkas/Llama-2-ko-7b-Chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "06317844-fe7a-484a-81e2-2f914efb6874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e9c5bd2afa4a32be3aae90d792f9eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_id, device_map={\"\": 0},torch_dtype=torch.float16, low_cpu_mem_usage=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "37f36f0c-04f5-4b09-a3f5-9a361cf22135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(46336, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=46336, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.use_cache = (True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c2951f4d-6b23-45ae-90d5-ed0ccba575da",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = '신용등급 알려줘'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4f7e2ac0-d9fe-47cb-8234-d0d6e36b5c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'한 신용등급은 현재 시점에서 제공하지 않으므로, 신용 등급과 관련한 질문을 다시 알려주시면 감사하겠습니다.</s>'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = gen(user_input, generation_config, model=model, tokenizer=tokenizer, device=device)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd09c175-5766-44f6-a96a-e750932740c1",
   "metadata": {},
   "source": [
    "#### PEFT Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96a24e5-d088-4249-8e4b-a756b6eba6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "config = AutoConfig.from_pretrained('meta-llama/Llama-2-7b-chat-hf', token=access_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559eed0d-e955-4d5f-b8ab-8c8ab09204e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
